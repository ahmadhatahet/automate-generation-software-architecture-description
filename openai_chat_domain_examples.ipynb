{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from prompts.StateDiagram import SYSTEM_PROMPT, user_message, example_message, feedback_message\n",
    "from utils import save_solution_puml, save_chat_json, save_chat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "base_path = Path().cwd()\n",
    "cpp_path = base_path / \"dishwasher_cpp/host\"\n",
    "\n",
    "diagrams_path = base_path / \"diagrams\"\n",
    "state_diagram_path = diagrams_path / \"state_diagram\"\n",
    "\n",
    "chat_path = base_path / \"chats\" / \"domain_examples\"\n",
    "chat_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xml_path = base_path / \"xml_data\"\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the example prompts for each core components\n",
    "core_components_file = base_path / \"core_components_openai.json\"\n",
    "core_components_list = json.loads(core_components_file.read_text())\n",
    "\n",
    "\n",
    "components_files = {}\n",
    "\n",
    "\n",
    "for target_component, code_files_raw in core_components_list.items():\n",
    "\n",
    "    # get code files for the target component\n",
    "    code_files = []\n",
    "    for c in code_files_raw:\n",
    "        code_files.append(base_path / c.replace(\"\\\\\", \"/\").replace(\".h\", \".cpp\"))\n",
    "\n",
    "    image_files = list(\n",
    "        (state_diagram_path / f\"{target_component}/ground_truth\").glob(\"*.jpg\")\n",
    "    )\n",
    "\n",
    "    if len(code_files) > 0:\n",
    "        components_files[target_component] = [code_files, image_files[0] if len(image_files) > 0 else None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format chatgpt response format\n",
    "class Solution(BaseModel):\n",
    "    solution: str = Field(\n",
    "        description=\"PlantUML solution that start with '@startuml' and end with '@enduml'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing component: abstractfactory\n",
      "Processing component: dishwasher\n",
      "Processing component: heater\n",
      "Processing component: jet\n",
      "Processing component: tank\n"
     ]
    }
   ],
   "source": [
    "for target_component, code_files_raw in core_components_list.items():\n",
    "\n",
    "    temp = components_files.pop(target_component)\n",
    "\n",
    "    # if cpp files do not exist, skip the component\n",
    "    if all(not c.is_file() for c in temp[0]):\n",
    "        print(f\"No code files found for component: {target_component}\")\n",
    "        continue\n",
    "\n",
    "    target_component = target_component.lower()\n",
    "    print(f\"Processing component: {target_component}\")\n",
    "\n",
    "    # User Prompt containing target cpp\n",
    "    user_prompt = user_message(temp[0])\n",
    "\n",
    "    # collect domain examples for other components\n",
    "    examples = [\n",
    "        example_message(v[0], v[1])\n",
    "        for c, v in components_files.items()\n",
    "        if v[1] is not None\n",
    "    ]\n",
    "    # flatten the examples list\n",
    "    examples = [e2 for e in examples for e2 in e]\n",
    "\n",
    "    # readd the target component to the components_files\n",
    "    components_files[target_component] = temp\n",
    "\n",
    "    # System Prompt\n",
    "    messages = SYSTEM_PROMPT + examples + user_prompt\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_API_BASE\"),\n",
    "    )\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=os.getenv(\"MODEL\"),\n",
    "        temperature=0.2,\n",
    "        messages=messages,\n",
    "        seed=SEED,\n",
    "        response_format=Solution,\n",
    "    )\n",
    "\n",
    "    # parse output\n",
    "    solution = json.loads(response.choices[0].message.content)[\"solution\"]\n",
    "\n",
    "    # save solution to puml file, render it, and save messages\n",
    "    chat_path_component = chat_path / target_component\n",
    "    chat_path_component.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    chat_filename = f\"state_general_{target_component}_{response.model}\"\n",
    "    date_postfix = datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "    save_solution_puml(\n",
    "        solution, state_diagram_path, target_component, response.model, date_postfix\n",
    "    )\n",
    "    save_chat_json(\n",
    "        messages, solution, response, chat_path_component, chat_filename, date_postfix\n",
    "    )\n",
    "    save_chat_text(messages, chat_path_component, chat_filename, date_postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
